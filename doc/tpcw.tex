\documentclass[twoside,11pt]{article}
\PassOptionsToPackage{hyphens}{url}
\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage[toc,page]{appendix}
\usepackage[table]{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}
\usepackage{subcaption}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix
}

\def\titl{Threaded Programming coursework I: benchmarking
  OpenMP schedules}

\title{\titl}

\author{\name Jonas Fassbender
        \email jonas@fassbender.dev}

\ShortHeadings{\titl}{Jonas Fassbender}
\firstpageno{1}


\begin{document}

\maketitle

\begin{abstract}
The benchmark of a scientific program presented in this
paper compares different OpenMP schedules based on how well
they increase the execution speed.

The scientific program contains two critical sections which
are suitable for speeding up with the OpenMP parallel
do-loop directive.
Both were tested with different schedules on four threads
and the fastest schedule for each section was determined.
In the second phase of the benchmark it was tested how well
the fastest schedules scale with less and more than four
threads.

The benchmark was executed on the Cirrus supercomputer with
exclusive access to one back end node.
The results of this benchmark show, that, based on the
schedule alone, the execution speed can increase by a
factor of 2.8.

This paper discusses the results of the benchmark and
furthermore raises questions concerning performance
anomalies and further optimizations.
Ideas for a follow-up benchmark testing changes
which could result in higher performance are given.

\end{abstract}

\begin{keywords}
OpenMP, parallel computing
\end{keywords}

\section{Introduction} % {{{

This paper documents the results of a benchmark performed
on a scientific program.
The program is written in the Fortran programming language
and performs element-wise computations on matrices and
vectors (using nested do-loops, not Fortran's array
operations).
It contains two of these matrix/vector operations---in this
paper called critical sections.

Both critical sections are suitable for speeding up with
OpenMP's loop construct, distributing the computation on
multiple threads of execution.
The loop construct provides the \texttt{schedule} clause,
which determines the division of the loop-iterations
among the OpenMP threads
\citep[see][Chapter 2.7.1]{openmp}.

The benchmark consists of two phases.
The goal of the first phase is to compare different
schedules of the OpenMP library and how they
effect the execution speed (measured in seconds) of the two
critical sections of the program.
The second phase provides data on how well the fastest
schedules for both critical sections scale with
different amounts of threads.

OpenMP version 4.5 was used and the benchmark was performed
on the back end of the Cirrus supercomputer
\citep[see][]{openmp, cirrus}.
The program was compiled with the Intel Fortran Compiler
(\texttt{ifort}) version 17.0.2, with the maximum
optimization provided (optimization level \texttt{O3})
\citep[see][]{ifort}.

First, this paper describes the conducted benchmark,
before presenting the results. At last the results are
discussed and a conclusion is drawn.

% }}}

\section{Experiment} % {{{
\label{sec:exp}

Let $n \in \mathbb{N}$ be a positive integer.
Let $A: n \times n$ and $B: n \times n$ be two matrices,
$A, B \in \mathbb{R}^n \times \mathbb{R}^n$.
Let $A(i, j); 1 \leq i, j \leq n$ be the element of $A$ in
the $i$th row and the $j$th column.
Every element in $A$ is initialized to 0 and every element
in $B$ is set according to:
$B(i, j) = \pi(i+j); i,j=1,\dots,n$.

The first critical section updates $A$:
\begin{align}
  \label{eq:cs1}
  A(i, j) = A(i, j) + \cos(B(i, j)); j=1,\dots,n; i=j,\dots,n.
\end{align}
$i$ depends on $j$, making the workload of every iteration
over $j$ smaller with increasing $j$.

\def\jmax{\vec{j}_{\text{max} }}

For the second critical section,
let $\vec{c}$ be the zero vector of size $n$.
Let $\jmax \in \mathbb{N}^n$ be another $n$-sized vector.
$\jmax$ is set to:
\begin{align}
  \label{eq:jmax}
  i=1,\dots,n: \jmax(i) =
  \begin{cases}
    n &\text{if } i \text{ mod }
       3\lfloor \frac{i}{30} \rfloor + 1 = 0 \\
    1 &\text{if } i \text{ mod }
       3\lfloor \frac{i}{30} \rfloor + 1 \neq 0
  \end{cases}.
\end{align}
The matrix $B'$ is set to $B'(i, j) = (ij + 1)n^{-2};
i,j = 1,\dots,n$.

The second critical section updates $\vec{c}$:
\begin{align}
  \label{eq:cs2}
  \vec{c}(i) = \sum_{j=1}^{\jmax(i)}\sum_{k=1}^{j}
    \vec{c}(i) + k\ln(B'(j, i))n^{-2}, i=1,\dots,n.
\end{align}

Since both (\ref{eq:cs1}) and (\ref{eq:cs2}) are
element-wise independent, the computation of every element
can be distributed over multiple processes.

Both critical sections are computationally asymmetric, both
having more demanding iterations at the beginning.
While---for the first critical section---the amount of
computations decrease linearly ($i$ depends on $j$---see
Equation~\ref{eq:cs1}), the amount of computation for the
second critical section drops even more for higher
iterations.

The amount of computations in the second critical section
is dependent on $\jmax$.
$\jmax(i)$ equals either 1 or $n$ and the
distribution of $\jmax(i) = n$ is asymmetric, since
the modulus in (\ref{eq:jmax}) changes depending on the
iteration $i$.
For example, $\jmax(i) = n$ for every element in the
interval $i \in (1, 29)$, while $\jmax(i) = n$ is true for
only 7 elements in $i \in (30, 59)$ and the amount keeps
decreasing with bigger $i$.

For both phases of the benchmark, $n$ was set to 729.
The different schedules used in the first phase
are:
\begin{itemize}
  \item Auto
  \item Static
  \item Static, Dynamic, Guided, all with different
    chunk sizes of: 1, 2, 4, 8, 16, 32, 64
\end{itemize}

The fastest schedules for the critical sections
are then run with 1, 2, 4, 6, 8, 12 and 16 threads during
the second phase of the benchmark.

Like stated in the introduction, both benchmark phases are
executed on the Cirrus back end with exclusive access to
one node.
Every schedule from phase one and every amount of
threads in phase two were executed 100 times and the
average and median walltime---in seconds---were measured
with the timing routine \texttt{omp\_get\_wtime}, provided
by OpenMP \citep[see][Chapter 3.4.1]{openmp}.
The average walltime was used as the decisive criteria for
execution speed.
The median was used as the secondary, tie-breaking
criteria.

% }}}

\section{Results} % {{{

% phase one fig, table {{{
\begin{table}
\begin{center}
\input{meta_loop_perm}
\caption{Results of phase one of the benchmark. Displayed
  are average and median walltime in seconds for every
  schedule for both critical sections. The fastest
  schedules are marked with a bold font-weight.}
\label{tab:p1}
\end{center}
\end{table}

\begin{figure}
\begin{subfigure}{\textwidth}
\begin{center}
% critical section 1 {{{
\begin{tikzpicture}[scale=1.5]
  \datavisualization[
    scientific axes=clean,
    visualize as line/.list={dynamic,guided, static},
    style sheet=vary dashing,
    style sheet=cross marks,
    dynamic={label in legend={text=Dynamic}},
    guided={label in legend={text=Guided}},
    static={label in legend={text=Static}},
    x axis={
      logarithmic,
      ticks={major={at={1,2,4,8,16,32,64} } },
      label={chunk size (\textit{log} scale)},
      grid={minor={at={2,4,8,16,32} }},
    },
    y axis={
      include value={0.45, 0.65},
      label={elapsed time in seconds},
    },
  ]
  data[set=dynamic]{
    x,  y
    1,  0.51
    2,  0.50
    4,  0.49
    8,  0.49
    16, 0.48
    32, 0.49
    64, 0.52
  }
  data[set=guided]{
    x,  y
    1,  0.49
    2,  0.49
    4,  0.49
    8,  0.49
    16, 0.49
    32, 0.50
    64, 0.50
  }
  data[set=static]{
    x,  y
    1,  0.53
    2,  0.51
    4,  0.52
    8,  0.52
    16, 0.54
    32, 0.56
    64, 0.62
  };
\end{tikzpicture}
% }}}
\caption{Critical section 1.}
\end{center}
\vspace{0.5cm}
\end{subfigure}
\begin{subfigure}{\textwidth}
\begin{center}
% critical section 2 {{{
\begin{tikzpicture}[scale=1.5]
  \datavisualization[
    scientific axes=clean,
    visualize as line/.list={dynamic,guided, static},
    style sheet=vary dashing,
    style sheet=cross marks,
    dynamic={label in legend={text=Dynamic}},
    guided={label in legend={text=Guided}},
    static={label in legend={text=Static}},
    x axis={
      logarithmic,
      ticks={major={at={1,2,4,8,16,32,64 } } },
      label={chunk size (\textit{log} scale)},
      grid={minor={at={2,4,8,16,32} }},
    },
    y axis={
      include value={2, 5.5},
      label={elapsed time in seconds},
    },
  ]
  data[set=dynamic]{
    x,  y
    1,  2.68
    2,  2.65
    4,  2.43
    8,  2.22
    16, 2.23
    32, 3.91
    64, 4.81
  }
  data[set=guided]{
    x,  y
    1,  5.33
    2,  5.33
    4,  5.33
    8,  5.33
    16, 5.33
    32, 5.33
    64, 5.33
  }
  data[set=static]{
    x,  y
    1,  3.96
    2,  2.84
    4,  2.60
    8,  2.37
    16, 3.17
    32, 4.84
    64, 5.37
  };
\end{tikzpicture}
% }}}
\caption{Critical section 2.}
\end{center}
\vspace{0.5cm}
\end{subfigure}
\caption{Plots on how the chunk size clause changes the
  execution speed of the Dynamic, Guided and Static
  schedules, for both critical sections.}
\label{fig:chunk_size}
\end{figure}
% }}}

Table~\ref{tab:p1} lists the average and median
walltime in seconds for the execution of the critical
sections, determined in phase one of the benchmark.

Sequential is not a schedule. It represents execution time
of the critical section in a sequential, not parallelized
manner.

One can see, for the first critical section the
schedules do not differentiate much concerning
the execution time.
Especially the Auto, Dynamic, $n$ and Guided, $n$
schedules produce results between $0.48$ and $0.52$
seconds of average execution time.
Static, $n$ performs slightly worse, all producing
an average execution time between $0.51$ and $0.56$
seconds, except Static, 64, which performs worse with an
average execution time of 0.62 seconds.
The Static schedule, which just splits the
iterations in \#threads (in this case four) approximately
equal chunks, performs the worst with an average execution
time of 0.83 seconds \citep[see][]{lecture}.

The best schedule for the first critical section
is Dynamic, 16, with an average and median execution time
of 0.48 seconds.

The schedules differ much more in execution time for
the second critical section, compared to the first one.
The difference between Dynamic, 8 (fastest) and Static
(slowest) is nearly 4 seconds, which makes Static
approximately 2.8 times slower than Dynamic, 8.
For comparison, the slowest schedule for
the first critical section is just approximately 1.7 times
slower than the fastest schedule.

Guided, $n$ and Auto, which were close to fastest for
the first critical section, perform worse on the
section critical section. All are approximately 2.4 times
slower than Dynamic, 8.

Static, $n$ fluctuates the most with different chunk
sizes $n$.
Static, 8, with 2.37 seconds average execution time, is
the third fastest schedule, while Static, 64
is the second slowest schedule with 5.37 seconds
average execution time.
The fluctuation of the average execution time, based
on different chunk sizes, can be seen in
Figure~\ref{fig:chunk_size}.

During the second phase of the benchmark the two scheduling
options resulting in the fastest average execution time
were tested with different amounts of threads.
The fastest schedule for the first critical
section was Dynamic, 16. For the second critical section
Dynamic, 8 resulted in the fastest average execution time.
Table~\ref{tab:p2} lists the average and median execution
time in seconds for both schedules when run
with 1, 2, 4, 6, 8, 12 and 16 threads.

Figure~\ref{fig:speedup} displays how much using more
threads gains in execution speed, compared to using just
one thread (sequential execution).
While for the first critical section the speedup of using
more threads grows linear, the execution time for the
second critical section stops being faster after 6
threads (see Table~\ref{tab:p2}, Figure~\ref{fig:speedup}).

% phase two fig, table {{{
\begin{table}
\begin{center}
\input{fastest_meta_loop_perm}
\caption{Results of phase two of the benchmark. Displayed
  are average and median walltime in seconds for the
  fastest schedules from phase one, for each
  critical section, executed with different amounts of
  threads.}
\label{tab:p2}
\end{center}
\end{table}

\begin{figure}
\begin{subfigure}{\textwidth}
\begin{center}
% critical section 1 {{{
\begin{tikzpicture}[scale=1.5]
  \datavisualization[
    scientific axes=clean,
    visualize as line,
    x axis={
      ticks={major={at={1,4,8,12,16}} },
      label={\#threads},
      grid={minor={at={4,8,12} }},
    },
    y axis={
      ticks={major={at={1,3,6,9,12}} },
      label={speedup ($\frac{T_1}{T_{\text{\#threads}} }$)},
    }
  ]
  data{
    x,  y
    1,  1
    2,  1.99
    4,  3.90
    6,  5.50
    8,  7.19
    12, 9.84
    16, 12.47
  };
\end{tikzpicture}
% }}}
\caption{Critical section 1.}
\end{center}
\vspace{0.5cm}
\end{subfigure}

\begin{subfigure}{\textwidth}
\begin{center}
% critical section 2 {{{
\begin{tikzpicture}[scale=1.5]
  \datavisualization[
    scientific axes=clean,
    visualize as line,
    x axis={
      ticks={major={at={1,4,8,12,16}} },
      label={\#threads},
      grid={minor={at={4,8,12} }},
    },
    y axis={
      include value={1,4.5},
      label={speedup ($\frac{T_1}{T_{\text{\#threads}} }$)},
    }
  ]
  data{
    x,  y
    1,  1
    2,  2.00
    4,  3.87
    6,  4.13
    8,  4.11
    12, 4.13
    16, 4.15
  };
\end{tikzpicture}
% }}}
\caption{Critical section 2.}
\end{center}
\vspace{0.5cm}
\end{subfigure}
\caption{Plots on how the execution speed varies with the
  amount of threads. The plots show how much faster more
  threads are, compared to just one thread.}
\label{fig:speedup}
\end{figure}
% }}}

% }}}

\section{Discussion} % {{{

There are several statements to make about why some
schedules outperform each other.
It should be noted here, that this chapter discusses some
ideas on why the performance of different schedules
differ.
Follow-up benchmarks would be needed to confirm or refute
them.

Both critical sections are asymmetric when it comes to the
computational distribution, both having more computations
to do during the first iterations.

This is not validated for the first critical section.
Guided, $n$, which splits the first iterations into bigger
chunks, reducing the chunk size over the iterations
\citep[see][]{lecture}, does perform nearly best (see
Table~\ref{tab:p1}).
On the other hand, Static and Static, $n$ are the worst
performing schedules. This is suggestive of an asymmetry
in the computational distribution.
A factor, which could regulate the asymmetry of the
first critical section, could be, that $\cos(x)$ takes
longer to compute for bigger $x$
(see Equation~\ref{eq:cs1}).
While less iterations over $i$ are needed in
(\ref{eq:cs1}) with bigger $j$, $B(i, j)$ contains bigger
values, which could lead to $\cos(x)$ taking longer to
compute.
This could easily be solved by
setting $B(i, j) = B(i, j) \mod 2\pi$, since if
$x \equiv y (\text{mod }2\pi)$, then $\cos(x) = \cos(y)$
(follows from the fact that $\cos(x)$ has $2\pi$ as its
period, see e.g. \citet{trig}).
This statement needs to be validated by a follow-up
benchmark.

The asymmetry of the second critical section on the other
hand, is validated by the bad performance of the Dynamic,
$n$ schedules.
This is the reason why all produce the same average
execution time.
The first iterations are the bottleneck of the second
critical section.

The high imbalance is also the reason the Static, Dynamic
and Guided schedules with higher chunk sizes are
outperformed by smaller ones, since the better the first
iterations are distributed among multiple threads, the
faster the imbalance---the bottleneck---gets executed.

On the other hand, Static, 1 and Dynamic, 1 are again
worse than bigger chunk sizes (2, 4, 8).
This could be the result of the higher amounts of context
switching and higher costs for scheduling the execution.

The imbalance is also the reason for the lack of speedup
with more threads for the Dynamic, 8 schedule, determined
as fastest for the second critical section during phase one
(see Table~\ref{tab:p2}, Figure~\ref{fig:speedup}).
The Dynamic schedule with a smaller chunk size would
probably scale better with more threads, which, again,
must be validated by a follow-up benchmark.

% }}}

\section{Conclusion} % {{{

The benchmark shows, using an optimized schedule for
a certain problem can increase the performance drastically.

The fastest schedule for the first critical section is
approximately 1.7 times faster than the slowest.
The second critical section---more imbalanced than the
first---produces even more severe performance differences,
the fastest schedule being approximately 2.8 times faster
than the slowest.

Even though better schedules were determined, this
benchmark raises questions about itself, which should be
addressed in a follow-up benchmark.
This could lead to even better performing schedules.

The results for the first critical section suggests, that
later iterations are more computationally expensive than
they seem, or at least that Dynamic, $n$ does not suffer
a performance decrease by the asymmetry of the
computational distribution, which seems strange.
The possibility of $\cos(x)$ taking longer to compute for
bigger was mentioned, which needs to be validated.

The other question is, if for the second critical section
schedules with smaller chunk sizes scale better (because
they better divide the bottleneck created by the
computational imbalance over the iterations) with more
threads than the best schedule determined on four, like
done in this benchmark.

% }}}

\bibliography{tpcw.bib}

\end{document}
