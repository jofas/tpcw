\documentclass[twoside,11pt]{article}
\PassOptionsToPackage{hyphens}{url}
\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage[toc,page]{appendix}
\usepackage[table]{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}
\usepackage{subcaption}
\usepackage{diagbox}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix,
    plotmarks
}

\usepgfplotslibrary{fillbetween, statistics}

\pgfplotsset{
  compat=1.3,
  every non boxed x axis/.style={
  enlarge x limits=false,
  x axis line style={}%-stealth},
  },
  every boxed x axis/.style={},
  every non boxed y axis/.style={
  enlarge y limits=false,
  y axis line style={}%-stealth},
  },
  every boxed y axis/.style={},
}

\def\titl{Threaded Programming coursework II: the affinity
  schedule for scheduling the OpenMP loop construct}

\title{\titl}

\author{}

\ShortHeadings{B160509}{B160509}
\firstpageno{1}


\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\begin{keywords}
Scientific programming, parallelization,
performance optimization, OpenMP
\end{keywords}

\section{Introduction} % {{{

OpenMP version 4.5 supports various scheduling options for
its loop construct, for example \texttt{static},
\texttt{dynamic} or \texttt{guided}
\citep[see][Chapter 2]{omp}.
This paper presents an alternative schedule, called the
affinity schedule.
The affinity schedule combines some properties of the three
above mentioned scheduling options into one schedule.

This paper is a follow-up of a benchmark presented in
\citet{b1}.
\citet{b1} presents a scientific program written in the
Fortran programming language, containing two loops
performing matrix and vector operations.
These two loops were parallelized using built-in scheduling
options of OpenMP version 4.5 and then benchmarked in order
to determine the best schedule for the two loops.
The here presented affinity schedule is benchmarked the
same way, as are the built-in schedules in \citet{b1}.

This paper begins by describing two versions of the
affinity schedule. Afterwards the benchmark is described
and its results are presented. The benchmark of the
affinity schedule is then compared to the best schedule for
both loops determined in \citet{b1}.
At last the results are discussed and a conclusion is
drawn.

% }}}

\section{Method} % {{{

Like stated in the previous chapter, the affinity schedule
combines properties from the already built-in schedules
\texttt{static}, \texttt{dynamic} and \texttt{guided}.
Let $n$ be the amount of iterations of the loop the
instance of the affinity schedule is applied to and let $p$
be the amount of OpenMP threads.
The affinity schedule splits the $n$ iterations of the
loop into $p$ splits, all having approximately
$\frac{n}{p}$ iterations.
This is the same way the \texttt{static} schedule splits
the iterations, if no chunk size is provided
\citep[see][Chapter 2]{omp}.

Every split is owned by one OpenMP thread. But, unlike the
\texttt{static} schedule, the split is not executed as a
whole.
Rather, it is split again into smaller chunks, which are
executed after another.
The size of the chunks gets smaller, the more iterations
of the split are already executed:
\begin{align*}
  chunk\ size := \lceil remaining\ iterations \cdot p^{-1} \rceil.
\end{align*}

The decreasing chunk sizes are a property of the
\texttt{guided} schedule.
While the \texttt{guided} schedule takes all $n$ iterations
and dynamically assigns the splits in a first come first
serve order to the OpenMP threads (like the
\texttt{dynamic} schedule), the affinity schedule
uses the same property of decreasing chunk size, just local
to each split \citep[see][Chapter 2]{omp}.

Once a thread has finished all chunks of the split it owns,
it is not simply idle until all threads have finished their
splits.
Instead, the thread determines the split that has still the
most iterations left and takes the next chunk from it.
Therefore, once the owned split of a thread is finished,
the thread changes from behaving like it is part of the
\texttt{guided} scheduling strategy on the local split to
being \texttt{dynamic} in the fact that the split with the
most iterations left is dynamically executed by all threads
that have finished their split, plus the owner thread.

The affinity schedule therefore combines the strengths,
concerning the execution speed, of the aforementioned
built-in schedules over the phases of its execution.
It starts executing like the \texttt{static} schedule,
which produces very low overhead of synchronization (in
fact none, but the affinity schedule must synchronize
access to the split), moving over to a \texttt{dynamic}
scheduling strategy,
which has a higher overhead of synchronization, but
produces no idle threads.
This effectively reduces the synchronization overhead of
the \texttt{dynamic} schedule while also removing the
idleness of threads during the execution with the
\texttt{static} schedule.

The affinity schedule is a shared object among each thread.
It contains the amount of OpenMP threads $p$, determined
by \texttt{omp\_get\_num\_threads}, an array of size $p$
containing the splits and an array of the same size
containing OpenMP locks (integers of the kind
\texttt{omp\_lock\_kind}).
The locks are used for the synchronization of the access
to a split \citep[see][Chapter 3]{omp}.
Every OpenMP thread has a unique id $p_i$ from the sequence
$1,2,\dots,p$ and owns the split from the split array at
the index $p_i$.\footnote{%
  OpenMP's \texttt{omp\_get\_thread\_num} function is used
  for determining the id.
  \texttt{omp\_get\_thread\_num} returns ids from the
  sequence $0,1,\dots,p-1$. Since the id is used to access
  splits from the splits array of the affinity schedule and
  since Fortran arrays by default start indexing at 1, the
  returned id by \texttt{omp\_get\_thread\_num} is
  incremented by 1 \citep[see][Chapter 3]{omp}.}
A split is nothing but a tuple of two non-negative
integers. The two elements are the remaining iterations of
the split and the index where the next chunk taken from the
split starts.
A chunk is a synonym for a closed interval $[i, j]$, which
is a subset of the whole loop executed by the affinity
schedule, which---in this case---can be thought of as the
interval $[1, n]$.
The thread that gets the chunk $[i, j]$ executes the loop
from $i$ to $j$, inclusively, before taking the next chunk,
either from its split, or, if the split is already
finished, from the split with the most iterations left.

The affinity schedule provides two methods as its
interface, called by each thread.
The methods are \texttt{init} and \texttt{take}.
\texttt{init} initializes everything and returns the id
to every thread.
It sets $p$, allocates the split and the lock array and
initializes the content of both.
While allocation and the setting of $p$ are done by a
single thread, the initialization of the splits and locks
are done by every thread concurrently.
That means, the thread with id $p_i$ initializes the split
in the split array at index $p_i$.
The same is done with the lock in the lock array, simply
by calling \texttt{omp\_init\_lock}
\citep[see][Chapter 3]{omp}.
Every thread must leave \texttt{init}, only after all
threads have finished initializing their split and lock.
Otherwise, if a very fast thread has already finished his
split, while other threads are still not finished with
initializing, the already finished thread could possibly
access an uninitialized split, which would result in a
segmentation fault.
Therefore, at the end of \texttt{init}, all threads are
synchronized with a \texttt{barrier}
\citep[see][Chapter 2]{omp}.

TODO: take
% interface schedule, intricacies

This paper describes two different versions of the affinity
schedule.
The versions differ in their strategy for determining the
split, which has the most iterations still to do.
The two versions are called \texttt{naive} and
\texttt{queue}.
The \texttt{naive} affinity schedule simply uses brute
force, iterating every split and find the one with the most
iterations left, while the \texttt{queue} affinity schedule
uses a priority queue based on the max-heap data structure,
which is constantly updated, every time a chunk is taken
from the split \citep[for the max-heap data structure see
e.g.][Chapter 6]{cormen}.


% describe the two different versions
% first naive then queue

% pseudo code
% graph ?? Probably not

% describe benchmark
%

% }}}

\section{Results} % {{{

% }}}

\section{Discussion} % {{{

% }}}

\section{Conclusion} % {{{

% }}}

\bibliography{tpcw.bib}

\end{document}
