\documentclass[twoside,11pt]{article}
\PassOptionsToPackage{hyphens}{url}
\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage[toc,page]{appendix}
\usepackage[table]{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}
\usepackage{subcaption}
\usepackage{diagbox}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix,
    plotmarks
}

\usepgfplotslibrary{fillbetween, statistics}

\pgfplotsset{
  compat=1.3,
  every non boxed x axis/.style={
  enlarge x limits=false,
  x axis line style={}%-stealth},
  },
  every boxed x axis/.style={},
  every non boxed y axis/.style={
  enlarge y limits=false,
  y axis line style={}%-stealth},
  },
  every boxed y axis/.style={},
}

\def\titl{Threaded Programming coursework II: the affinity
  schedule for scheduling the OpenMP loop construct}

\title{\titl}

\author{}

\ShortHeadings{B160509}{B160509}
\firstpageno{1}


\begin{document}

\maketitle

\begin{abstract}
This paper describes an alternative schedule for scheduling
OpenMP's loop construct, the affinity schedule.
The affinity schedule tries to add positive properties of
different built-in schedules together into one schedule.
This paper is a follow-up on a benchmark presented in
\citet{b1}.

Two versions of the affinity schedule are presented and
compared against each other.
This paper describes in detail how the two versions of the
affinity schedule are implemented.
Afterwards they are benchmarked and compared against well
performing built-in schedules determined in \citet{b1}.

While the two versions of the affinity schedule perform
well for the most parts and sometimes even outperform
the built-in schedules, this paper comes to the conclusion
that the affinity schedule is simply too complex and
contains too much synchronization, which hurts its
scalability.
An idea for a better performing affinity schedule is
presented.
\end{abstract}

\begin{keywords}
Scientific programming, parallelization,
performance optimization, OpenMP
\end{keywords}

\section{Introduction} % {{{

OpenMP version 4.5 supports various scheduling options for
its loop construct, for example \texttt{static},
\texttt{dynamic} or \texttt{guided}
\citep[see][Chapter 2]{omp}.
This paper presents an alternative schedule, called the
affinity schedule.
The affinity schedule combines some properties of the three
above mentioned scheduling options into one schedule,
trying to combine the strengths of these schedules and
canceling out their weaknesses.

This paper is a follow-up of a benchmark presented in
\citet{b1}.
\citet{b1} presents a scientific program written in the
Fortran programming language, containing two loops
performing matrix and vector operations.
These two loops were parallelized using built-in scheduling
options of OpenMP version 4.5 and then benchmarked in order
to determine the best schedule for the two loops.
The here presented affinity schedule is benchmarked the
same way, as are the built-in schedules in \citet{b1}.

This paper begins by describing two versions of the
affinity schedule. Afterwards the benchmark is described
and its results are presented. The benchmark of the
affinity schedule is then compared to the best schedule for
both loops determined in \citet{b1}.
At last the results are discussed and a conclusion is
drawn.

% }}}

\section{Method} % {{{
\label{sec:method}

Like stated in the previous chapter, the affinity schedule
combines properties from the already built-in schedules
\texttt{static}, \texttt{dynamic} and \texttt{guided}.
Let $n$ be the amount of iterations of the loop the
instance of the affinity schedule is applied to and let $p$
be the amount of OpenMP threads.
The affinity schedule splits the $n$ iterations of the
loop into $p$ splits, all having approximately
$\frac{n}{p}$ iterations.
This is the same way the \texttt{static} schedule splits
the iterations, if no chunk size is provided
\citep[see][Chapter 2]{omp}.

Every split is owned by one OpenMP thread. But, unlike the
\texttt{static} schedule, the split is not executed as a
whole.
Rather, it is split again into smaller chunks, which are
executed after another.
The size of the chunks gets smaller, the more iterations
of the split are already executed:
\begin{align*}
  chunk\ size := \lceil remaining\ iterations \cdot p^{-1} \rceil.
\end{align*}

The decreasing chunk sizes are a property of the
\texttt{guided} schedule.
While the \texttt{guided} schedule takes all $n$ iterations
and dynamically assigns the splits in a first come first
serve order to the OpenMP threads (like the
\texttt{dynamic} schedule), the affinity schedule
uses the same property of decreasing chunk size, just local
to each split \citep[see][Chapter 2]{omp}.

Once a thread has finished all chunks of the split it owns,
it is not simply idle until all threads have finished their
splits (a weakness of the \texttt{static} schedule).
Instead, the thread determines the split that has still the
most iterations left and takes the next chunk from it.
Therefore, once the owned split of a thread is finished,
the thread changes from behaving like it is part of the
\texttt{guided} scheduling strategy on the local split to
being \texttt{dynamic} in the fact that the split with the
most iterations left is dynamically executed by all threads
that have finished their split, plus the owner thread,
freeing the owner thread to become \texttt{dynamic} as
well.
This is continued until all splits are executed.

The affinity schedule therefore combines the strengths,
concerning the execution speed, of the aforementioned
built-in schedules over the phases of its execution.
It starts executing like the \texttt{static} schedule,
which produces very low overhead of synchronization (in
fact none, but the affinity schedule must synchronize
access to the splits), moving over to a \texttt{dynamic}
scheduling strategy,
which has a higher overhead of synchronization, but
produces no idle threads.
This should effectively reduce the synchronization overhead
of the \texttt{dynamic} schedule while also removing the
idleness of threads during the execution with the
\texttt{static} schedule.
In the following chapters, this property is discussed.
Determining the split with the most iterations left is no
trivial task and involves a lot of synchronization, at
least in the versions of the affinity schedule here
presented.

The affinity schedule is a shared object among each thread.
It contains the amount of OpenMP threads $p$, determined
by \texttt{omp\_get\_num\_threads}, an array of size $p$
containing the splits and an array of the same size
containing OpenMP locks (integers of the kind
\texttt{omp\_lock\_kind}).
The locks are used for the synchronization of the access
to a split \citep[see][Chapter 3]{omp}.

Every OpenMP thread has a unique id $p_i$ from the sequence
$1,2,\dots,p$ and owns the split from the split array at
the index $p_i$.\footnote{%
  OpenMP's \texttt{omp\_get\_thread\_num} function is used
  for determining the id.
  \texttt{omp\_get\_thread\_num} returns ids from the
  sequence $0,1,\dots,p-1$. Since the id is used to access
  splits and locks from the arrays of the affinity schedule
  and since Fortran arrays by default start indexing at 1,
  the returned id by \texttt{omp\_get\_thread\_num} is
  incremented by 1 \citep[see][Chapter 3]{omp}.}
A split is nothing but a tuple of two non-negative
integers. The two elements are the remaining iterations of
the split and the index where the next chunk taken from the
split starts.
A chunk is a synonym for a closed interval $[i, j]$, which
is a subset of the whole loop executed by the affinity
schedule, which---in this case---can be thought of as the
interval $[1, n]$.
The thread that gets the chunk $[i, j]$ executes the loop
from $i$ to $j$, inclusively, before taking the next chunk,
either from its split, or, if the split is already
finished, from the split with the most iterations left.

\begin{algorithm}
  \caption{: executing a loop with the affinity schedule}
  \label{alg:main}

  \begin{algorithmic}[1]
    \STATE{start OpenMP parallel region with schedule being shared}
    \STATE{initialize schedule with \texttt{init}}
    \WHILE{$true$}
      \STATE{get interval from \texttt{take}(schedule, $p_i$)}
      \IF {the interval is valid}
        \STATE{execute subset of loop defined by the interval}
      \ELSE
        \STATE{exit while loop}
      \ENDIF
    \ENDWHILE
    \STATE{end OpenMP parallel region}
  \end{algorithmic}
\end{algorithm}

The affinity schedule provides two methods as its
interface, called by each thread.
The methods are \texttt{init} and \texttt{take}.
\texttt{init} initializes everything and returns the id
to every thread.
It sets $p$, allocates the split and the lock array and
initializes the content of both.
While allocation and the setting of $p$ are done by a
single thread\footnote{The \texttt{single} construct is
  used for this \citep[see][Chapter 2]{omp}.},
the initialization of the splits and locks
are done by every thread concurrently.
That means, the thread with id $p_i$ initializes the split
in the split array at index $p_i$.
The same is done with the lock in the lock array, simply
by calling \texttt{omp\_init\_lock}
\citep[see][Chapter 3]{omp}.
Every thread must leave \texttt{init}, only after all
threads have finished initializing their split and lock.
Otherwise, if a very fast thread has already finished his
split, while other threads are still not finished with
initializing, the already finished thread could possibly
access an uninitialized split, which would result in a
segmentation fault.
Therefore, at the end of \texttt{init}, all threads are
synchronized with a \texttt{barrier}
\citep[see][Chapter 2]{omp}.
Algorithm~\ref{alg:init} shows the \texttt{init} method.

\begin{algorithm}
  \caption{: \texttt{init}}
  \label{alg:init}

  \begin{algorithmic}[1]
    \STATE{set the $p_i$ of the calling thread to
      \texttt{omp\_get\_thread\_num} + 1}
    \STATE{enter OpenMP \texttt{single} block}
    \STATE{set $p$ with \texttt{omp\_get\_num\_threads}}
    \STATE{allocate the arrays for the splits and for the locks}
    \STATE{leave OpenMP \texttt{single} block}
    \STATE{initialize split and lock in the arrays at the index
      $p_i$}
    \IF{the schedule is a \texttt{queue} affinity schedule}
      \STATE{initialize the priority queue and its lock as well}
    \ENDIF
    \STATE{wait here till all threads have finished (realized
      with the OpenMP \texttt{barrier} structure)}
  \end{algorithmic}
\end{algorithm}

After a thread leaves \texttt{init}, it starts executing
chunks.
The thread does not know or care, where the chunks are
coming from, as long as they are valid.
It has no concept of owning a split.
The ownership relationship is only implemented in the
affinity schedule and a thread is reduced to its id and
the basic task of executing chunks.
Algorithm~\ref{alg:main} shows how the threads execute
when the affinity schedule is used.

In order for a thread to get the next chunk for execution,
it calls the \texttt{take} method of the affinity schedule
instance.
\texttt{take} takes the id of the thread as an argument.
It first looks at the split owned by the calling thread.
If it has already finished, it instead tries to get a
chunk of the split with the most iterations left.
Before accessing any split, the corresponding lock in the
lock array must be set with \texttt{omp\_set\_lock}, so
the current thread has exclusive access to the split and
race conditions are avoided.
After a split is accessed, the lock is unset with
\texttt{omp\_unset\_lock} \citep[see][Chapter 3]{omp}.

The result of \texttt{take}, on an abstract level, can be
thought of as an instance of an algebraic sum type, which
returns either a valid interval or nothing
\citep[for sum types see e.g.][]{sum_types}.\footnote{%
  In languages like Rust and Swift this type is called the
  \texttt{Option} type \citep[see][]{rust, swift}.
  Fortran unfortunately does not support sum types. So
  the actual implementation is a wrapper around an interval
  with a boolean field indicating, whether the returned
  interval is valid or not.}
If nothing is returned (in this case the boolean field
\texttt{is\_none} would be true), then all splits have
finished and the thread can stop its execution
(see Algorithm~\ref{alg:main}, lines 5ff).
Otherwise it executes the returned interval and calls
\texttt{take} again, until it returns nothing
(see Algorithm~\ref{alg:main}, line 4).
Algorithm~\ref{alg:take} displays \texttt{take}.

\begin{algorithm}
  \caption{: \texttt{take}}
  \label{alg:take}

  \begin{algorithmic}[1]
    \STATE{set the lock in the lock array at index $p_i$}
    \IF{the split in the split array at index $p_i$ is
        \textbf{not} finished}
      \STATE{take chunk from the split at index $p_i$}
      \STATE{unset the lock at index $p_i$}
    \ELSE
      \STATE{unset the lock at index $p_i$}
      \STATE{get id of the thread owning the split with the
        most iterations left $p_{most}$ (either
        \texttt{naive} or with the \texttt{queue})}
      \STATE{set the lock in the lock array at index $p_{most}$}
      \STATE{take chunk from the split at index $p_{most}$}
      \STATE{unset the lock at index $p_{most}$}
    \ENDIF
    \RETURN{the taken chunk (can be nothing if the split
      with the most remaining iterations is finished)}
  \end{algorithmic}
\end{algorithm}

This paper describes two different versions of the affinity
schedule.
The versions differ in their strategy for determining the
split, which has the most iterations still to do.
The two versions are called \texttt{naive} and
\texttt{queue}.
The \texttt{naive} affinity schedule simply uses brute
force, iterating every split and find the one with the most
iterations left, while the \texttt{queue} affinity schedule
uses a priority queue based on the max-heap data structure,
which is constantly updated, every time a chunk is taken
from the split \citep[for the max-heap data structure see
e.g.][Chapter 6]{cormen}.

Both implementations have weaknesses.
While the \texttt{naive} version is, in theory slow,
because the lookup requires the calling thread to iterate
all splits, the priority queue used by the \texttt{queue}
affinity schedule is not thread-safe and therefore requires
a lot of synchronization, which reduces its in theory
faster lookup of the split with the most iterations left.
For this reason, the access to the priority queue must be
locked with an additional lock, so only
a single thread can access the priority queue at any given
moment.
The performance of both versions is compared in the
following chapters.

The priority queue provides two main methods as its
interface (and some more minor methods for constructing
it), \texttt{max\_element} and \texttt{decrease\_key}.
It consists of three arrays, all the size of $p$.
One array is the heap, containing the remaining iterations
of each split \citep[see][Chapter 6]{cormen}.
The second array---the lookup array---contains the index of
the split, corresponding to its key in the heap.
For example, if the split owned by the thread $p_3$ is the
split with the most remaining iterations of 15, the first
element of the heap would be 15 and the first element of
the lookup array would be 3.
The lookup array is for fast access to the split that has
the most iterations left.
The \texttt{max\_element} method simply returns the first
element of the lookup array.

After a thread takes a chunk from a split, the remaining
iterations of the split are decreased.
The decreased key must be updated in the priority queue.
The third array of the priority queue is the reverse
lookup array for fast access during the
\texttt{decrease\_key} operations.
Lets continue the example from above: a thread takes the
next chunk of size 3 from the split owned by $p_3$,
respectively decreasing the remaining iterations of that
split to 12.
\texttt{decrease\_key} takes $p_3$ and 12 as its arguments.
Now, in order to find the heap element which corresponds
to $p_3$, we could iterate over the elements of the
lookup array until we find the element that is equal to
$p_3$.
This is not very elegant or efficient, so the priority
queue maintains the reverse lookup array.
So, instead of iterating over the lookup array, instead
the reverse lookup array at index $p_3$ returns the
position of $p_3$ in the heap, in this example 1 (see
Algorithm~\ref{alg:dk}, line 1).
Afterwards the $15$ in the heap at index 1 is decreased to
12 and the heap property is restored (if 12 is smaller than
any of its children, they are recursively swapped until no
children are bigger than 12 (see Algorithm~\ref{alg:dk},
lines 8ff)).
Finished splits (where remaining iterations are zero),
are automatically removed from the heap (see
Algorithm~\ref{alg:dk}, lines 2ff).

\begin{algorithm}
  \caption{: \texttt{decrease\_key}($p_i$, $key$)}
  \label{alg:dk}

  \begin{algorithmic}[1]
    \STATE{get heap and lookup index $i$ from the reverse
      lookup array at index $p_i$}
    \IF{$key = 0$}
      \STATE{swap places with the last element of the heap}
      \STATE{decrease the heap size}
      \STATE{restore heap property \citep[see][Chapter 6]{cormen}}
    \ELSE
      \STATE{decrease value in heap at index $i$ to $key$}
      \WHILE{the decreased element has children that are
          greater}
        \STATE{swap the element with its biggest child}
      \ENDWHILE
    \ENDIF
  \end{algorithmic}
\end{algorithm}

The benchmark presented in the following chapters is the
same as the second part of the benchmark presented in
\citet{b1}.
The two loops of the scientific program are executed by
affinity schedule instances on a back end node of the
Cirrus supercomputer with exclusive access
\citep[see][]{cirrus}.
The program is run with 1, 2, 4, 6, 8, 12 and 16 OpenMP
threads, five times per amount of threads.
The timing of the execution is done with the
\texttt{omp\_get\_wtime} routine
\citep[see][Chapter 3]{omp}.

Both versions of the affinity schedule are tested. They
are compared against each other and the results of the best
built-in schedules for the two loops of the program,
determined by the benchmark from \citet{b1}.

% }}}

\section{Results} % {{{

\begin{figure} % actual speed {{{

\begin{subfigure}{\linewidth} % {{{
  \begin{center}
    \begin{tikzpicture}[scale=1.75] %{{{
  \datavisualization[
    scientific axes=clean,
    visualize as line/.list={d, d2, d3},
    style sheet=vary dashing,
    style sheet=strong colors,
    style sheet=cross marks,
    d={
      label in legend={text={\texttt{naive}} },
    },
    d2={
      label in legend={text={\texttt{queue}} },
    },
    d3={
      label in legend={text={\texttt{dynamic, 16}} },
    },
    y axis={
      %ticks={major={at={1,2,4,8,16}} },
      label={average time in seconds},
    },
    x axis={
      logarithmic,
      ticks={major={at={1,2,4,8,16}} },
      label={amount of threads $t$ (\textit{log} scale)},
      grid={minor={at={2,4,8} }},
    },
  ]
  data[headline={x, y}, read from file=data/time_naive_1.csv, set=d]
  data[headline={x, y}, read from file=data/time_queue_1.csv, set=d2]
  data[headline={x, y}, set=d3] {
    1,  1.87
    2,  0.94
    4,  0.48
    6,  0.34
    8,  0.26
    12, 0.19
    16, 0.15
  }
  ;
    \end{tikzpicture} % }}}
  \end{center}
  \caption{Loop 1.}
  \label{subfig:time_loop1}
  \vspace{1cm}
\end{subfigure} % }}}

\begin{subfigure}{\linewidth} % {{{
  \begin{center}
    \begin{tikzpicture}[scale=1.75] %{{{
  \datavisualization[
    scientific axes={clean},
    visualize as line/.list={d, d2, d3},
    style sheet=vary dashing,
    style sheet=strong colors,
    style sheet=cross marks,
    d={
      label in legend={text={\texttt{naive}} },
    },
    d2={
      label in legend={text={\texttt{queue}} },
    },
    d3={
      label in legend={text={\texttt{dynamic, 8}} },
    },
    y axis={
      %ticks={major={at={1,2,4,8,16}} },
      label={average time in seconds},
    },
    x axis={
      logarithmic,
      ticks={major={at={1,2,4,8,16}} },
      label={amount of threads $t$ (\textit{log} scale)},
      grid={minor={at={2,4,8} }},
    },
  ]
  data[headline={x, y}, read from file=data/time_naive_2.csv, set=d]
  data[headline={x, y}, read from file=data/time_queue_2.csv, set=d2]
  data[headline={x, y}, set=d3] {
    1,  8.59
    2,  4.30
    4,  2.22
    6,  2.08
    8,  2.10
    12, 2.08
    16, 2.07
  }
  ;
    \end{tikzpicture} % }}}
  \end{center}
  \caption{Loop 2.}
  \label{subfig:time_loop2}
  \vspace{1cm}
\end{subfigure} % }}}

\caption{Average execution time in seconds, taken per
  tested amount of threads.}
\label{fig:time}
\end{figure} % }}}

The results here presented are grouped by each of the two
parallelized loops of the benchmarked scientific program.
The loops are thoroughly described and analyzed in
\citet{b1}.
Both loops are asymmetric when it comes to the distribution
of the computational complexity.
Both loops are more complex at the beginning, which means
chunks closer to 1 take longer to finish then chunks closer
to $n$. $n = 729$ for the benchmark \citep[see][]{b1}.
While for the first loop complexity decreases linearly,
for the second loop the complexity at the beginning is much
higher and decreases more extreme \citep[see][]{b1}.

Figure~\ref{subfig:time_loop1} shows the average execution
time for both affinity schedule versions and the best
built-in schedule determined in \citet{b1} for loop 1:
\texttt{dynamic, 16}.
Figure~\ref{subfig:time_loop2} shows the same for the
second loop.

Both best built-in versions come from a preselection of
schedules tested on the two loops of the scientific
program.
The two best schedules, \texttt{dynamic, 16} for the first
and \texttt{dynamic, 8} for the second loop, were selected
based on their performance when run with 4 threads.
\citet{b1} argues, that, especially for the second loop,
a \texttt{dynamic} schedule with a smaller chunk size,
would scale better.

One can see in Figure~\ref{subfig:time_loop1} that the
\texttt{naive} affinity schedule performs better than the
\texttt{queue} version.
For both loops, the \texttt{queue} affinity schedule
performs as good as the \texttt{naive} affinity schedule
for the most parts, except for the tests with 16 OpenMP
threads, where the performance of the \texttt{queue}
affinity schedule very much deteriorates, resulting in
timings worse than using it with 8 threads.
The \texttt{naive} affinity schedule on the other hand
has no such scaling problems.
It produces a monotonic increasing function over the
different amounts of threads tested.

When the results of the affinity schedules are compared to
the best built-in schedules for both loops, they behave
exactly the opposite for both loops.
For the first loop, the two affinity schedule versions are
faster than the built-in schedule for lower amounts of
threads (1, 2, 4, 6).
With 8, 12 and 16 threads \texttt{dynamic, 16} performs
better than the affinity schedules.
For the second loop, this behavior changes.
\texttt{dynamic, 8} performs better with 1, 2, 4 and 6
threads, while the affinity schedules outperform it with
8, 12 and 16 threads.

\begin{figure} % speedup {{{

\begin{subfigure}{\linewidth} % {{{
  \begin{center}
    \begin{tikzpicture}[scale=1.75] %{{{
  \datavisualization[
    scientific axes=clean,
    visualize as line/.list={d1, d, d2, d3},
    style sheet=vary dashing,
    style sheet=strong colors,
    style sheet=cross marks,
    d1={
      label in legend={text={optimal}}
    },
    d={
      label in legend={text={\texttt{naive}} },
    },
    d2={
      label in legend={text={\texttt{queue}} },
    },
    d3={
      label in legend={text={\texttt{dynamic, 16}} },
    },
    y axis={
      logarithmic,
      ticks={major={at={1,2,4,8,16}} },
      label={speedup $\frac{\emptyset time_{t_1}}{\emptyset time_{t}}$ (\textit{log} scale)}
    },
    x axis={
      logarithmic,
      ticks={major={at={1,2,4,8,16}} },
      label={amount of threads $t$ (\textit{log} scale)},
      grid={minor={at={2,4,8} }},
    },
  ]
  data[headline={x, y}, read from file=data/speedup_naive_1.csv, set=d]
  data[headline={x, y}, read from file=data/speedup_queue_1.csv, set=d2]
  data[headline={x, y}, set=d3] {
    1,  1
    2,  1.99
    4,  3.90
    6,  5.50
    8,  7.19
    12, 9.84
    16, 12.47
  }
  data[headline={x, y}, set=d1] {
    1, 1
    2, 2
    4, 4
    6, 6
    8, 8
    12, 12
    16, 16
  }
  ;
    \end{tikzpicture} % }}}
  \end{center}
  \caption{Loop 1.}
  \label{subfig:speedup_loop1}
  \vspace{1cm}
\end{subfigure} % }}}

\begin{subfigure}{\linewidth} % {{{
  \begin{center}
    \begin{tikzpicture}[scale=1.75] %{{{
  \datavisualization[
    scientific axes={clean},
    visualize as line/.list={d1, d, d2, d3},
    style sheet=vary dashing,
    style sheet=strong colors,
    style sheet=cross marks,
    d1={
      label in legend={text={optimal}}
    },
    d={
      label in legend={text={\texttt{naive}} },
    },
    d2={
      label in legend={text={\texttt{queue}} },
    },
    d3={
      label in legend={text={\texttt{dynamic, 8}} },
    },
    y axis={
      logarithmic,
      ticks={major={at={1,2,4,8,16}} },
      label={speedup $\frac{\emptyset time_{t_1}}{\emptyset time_{t}}$ (\textit{log} scale)}
    },
    x axis={
      logarithmic,
      ticks={major={at={1,2,4,8,16}} },
      label={amount of threads $t$ (\textit{log} scale)},
      grid={minor={at={2,4,8} }},
    },
  ]
  data[headline={x, y}, read from file=data/speedup_naive_2.csv, set=d]
  data[headline={x, y}, read from file=data/speedup_queue_2.csv, set=d2]
  data[headline={x, y}, set=d3] {
    1,  1
    2,  2.0
    4,  3.87
    6,  4.13
    8,  4.11
    12, 4.13
    16, 4.15
  }
  data[headline={x, y}, set=d1] {
    1, 1
    2, 2
    4, 4
    6, 6
    8, 8
    12, 12
    16, 16
  }
  ;
    \end{tikzpicture} % }}}
  \end{center}
  \caption{Loop 2.}
  \label{subfig:speedup_loop2}
  \vspace{1cm}
\end{subfigure} % }}}

\caption{Speedup when using more threads. The speedup
  was calculated over the average of every measurement,
  grouped by the amount of threads.}
\label{fig:speedup}
\end{figure} % }}}

While Figure~\ref{fig:time} shows the performance of the
schedules, Figure~\ref{fig:speedup} shows their scaling
capabilities by displaying the speedup of each schedule
when using more threads.
Figure~\ref{subfig:speedup_loop1} shows that the affinity
schedules do not scale as well as \texttt{dynamic, 16} for
the first loop.
Again the deterioration of the \texttt{queue} affinity
schedule for 16 threads is shown.

During execution of the second loop,
Figure~\ref{subfig:speedup_loop2} shows that the affinity
schedules do not scale even close to optimal speedup for
2, 4 and 6 threads.
Their scaling increases rather rapidly from 4 to 12 threads
while decreasing again for 16 threads.
\texttt{dynamic, 8} scales close to optimal until 4 threads
and hits a plateau afterwards, which is the reason why
the affinity schedules outperform the built-in schedule
for more threads.

% }}}

\section{Discussion} % {{{

While the \texttt{queue} version of the affinity schedule
has the theoretical better sequential performance than the
\texttt{naive} affinity schedule, the fact that it is not
thread-safe and therefore must be made thread-safe by
mutual exclusion, clearly makes the priority queue a
bottleneck.
The synchronization really hurts the \texttt{queue}
affinity schedule, making its performance drop with higher
amounts of threads, wherefore the \texttt{naive} affinity
schedule outperforms it.

While for the first loop the performance difference between
the built-in schedule and both versions of the affinity
schedule is minimal, the affinity schedules---especially
the \texttt{naive} affinity schedule---out-perform the
\texttt{dynamic, 8} for the second loop with higher
threads.
This paper therefore underlines the claim made in
\citep{b1}, that a \texttt{dynamic} schedule with a lower
chunk size than 8 would scale better for the second loop,
because the tested affinity schedules provide a lower
chunk size for the most parts.

Overall is the usefulness of the affinity schedule
questionable, because more inner state must be maintained
(the shared splits instead of just the loop) and more
synchronization must be performed---once a thread has
finished its split---than during a truly \texttt{dynamic}
schedule.

% }}}

\section{Conclusion} % {{{

The affinity schedule's performance is comparable to the
built-in schedules tested in \citet{b1}, sometimes even
outperforming them.
But overall, the affinity schedule suffers from its
complexity and the necessarily accompanying
synchronization, because it tries to add the properties
of several built-in schedules into one schedule (see
Chapter~\ref{sec:method}).

If one were to continue looking into a better performing
affinity schedule, a good place to start would probably be
to build the \texttt{queue} affinity schedule on top of a
thread-safe priority queue.
This could remove its bottleneck and make it scale in a
nice way, because its overhead would be compensated by
a bigger amount of threads, where probably the
\texttt{naive} affinity schedule would suffer performance
drops.
% too complex, but still performs comparably to built-in
% schedules
%
% thread-safe version of the queue should be tried (maybe
% making performance better)

% }}}

\bibliography{tpcw.bib}

\end{document}
